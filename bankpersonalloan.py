# -*- coding: utf-8 -*-
"""BankPersonalLoan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15rRJfqpzfqJPVVNdOvU41M7Od-ruw6zH

##**Bank Personal Loan Modelling:-**

 Identifying the Potential
Customers for Loans.

**Domain:**

Banking and Marketing.

**Context:**
This case is about a bank (Thera Bank) which has a growing customer base. Majority of these customers
are liability customers (depositors) with varying sizes of deposits. The number of customers who are
also borrowers (asset customers) is quite small, and the bank is interested in expanding this base
rapidly to bring in more loan business and in the process, earn more through the interest on loans. In
particular, the management wants to explore ways of converting its liability customers to personal loan
customers (while retaining them as depositors).
A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over
9% success. This has encouraged the retail marketing department to devise campaigns with better
target marketing to increase the success ratio with minimal budget.

**Objective:**
The department wants to build a model that will help them identify the potential customers who have
a higher probability of purchasing the loan. This will increase the success ratio while at the same time
reduce the cost of the campaign.

###1. Import the required libraries and read the dataset.
"""

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

#mount the drive
from google.colab import drive
drive.mount('/content/drive')

bank_loan=pd.read_csv('/content/drive/MyDrive/Python/Python_Project/Project_5/Bank_Personal_Loan_Modelling.csv')

"""###2. Check the first few samples, shape, info of the data and try to familiarize yourself with different features."""

bank_loan.shape

bank_loan.head(2)

bank_loan.tail(2)

bank_loan.isna().sum()

bank_loan.info()

"""###3. Check if there are any duplicate records present in the dataset? If yes, drop them. and Drop the columns which you feel are redundant."""

bank_loan.drop_duplicates(inplace=True)

"""**(OR)**"""

# # Check for duplicate records
# duplicate_rows = bank_loan[bank_loan.duplicated()]

# # If there are duplicates, drop them
# if not duplicate_rows.empty:
    #bank_loan = bank_loan.drop_duplicates()

bank_loan.shape

bank_loan.drop(['Online','CD Account','Family','CreditCard'],axis=1,inplace=True)

bank_loan.shape

"""###4. Display the Five Point Summary and write your key findings."""

bank_loan.describe()

"""**5 Points Summary**

**Note:**Based on above code I took the values

**Ex: (Age Column)**

**1)Min:** 23.00 (Least value in the column)

**2)Q1:** 35.00 (25%  of the value in the column)

**3)Q2(Median):** 45.00 (Midean value in the column)

**4):Q3** 55.00 (75% of the value in the column)

**5)Max:** 67.00 ( Maximum value in the column)

###5. There are negative values in the variable 'Experience'. Convert them to non-negative values. (Hint: abs function)
"""

bank_loan['Experience']=bank_loan['Experience'].abs()

"""###6. Get the target column distribution and comment on the class distribution."""

bank_loan['Personal Loan'].value_counts()

plt.figure(figsize=(4,8))
plt.pie(bank_loan['Personal Loan'].value_counts(),labels=bank_loan['Personal Loan'].unique(), autopct='%1.1f%%')
plt.title("Percentage of Personal Loan")
plt.show()

bank_loan['Personal Loan'].value_counts().plot(kind='bar')
# Adding labels to the bars
for i, value in enumerate(bank_loan['Personal Loan'].value_counts()):
    plt.text(i, value + 1, str(value), ha='center', va='bottom')

"""###7. Store the target column (i.e.Personal Loan) in the y variable and the rest of the columns in the X variable."""

x=bank_loan.drop(['Personal Loan'],axis=1)
y=bank_loan['Personal Loan']

"""###8. Split the dataset into two parts (i.e. 70% train and 30% test). and standardize the columns using the z-score scaling approach."""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=2529)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Creating DataFrames from the standardized arrays (optional)
# X_train_standardized_df = pd.DataFrame(X_train_standardized, columns=X.columns)
# X_test_standardized_df = pd.DataFrame(X_test_standardized, columns=X.columns)

print(x_train)

print(x_test)

"""###9. Train and test a Logistic Regression model to predict the likelihood of a liability customer buying personal loans. Display the train and test accuracy scores."""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(x_train, y_train)

# Predictions on train and test sets
train_predictions = model.predict(x_train)
test_predictions = model.predict(x_test)

# Calculate accuracy scores
train_accuracy = accuracy_score(y_train, train_predictions)
test_accuracy = accuracy_score(y_test, test_predictions)

# Displaying the train and test accuracy scores
print("Train Accuracy:",train_accuracy)
print(f"Test Accuracy: {test_accuracy:.4f}")   #.4f means it won't give many decimal numbers 0.93666666666666666

"""###10. Print the confusion matrix and classification report for the model and write your conclusions on the results.

**Test data**
"""

from sklearn.metrics import confusion_matrix, classification_report

conf_matrix = confusion_matrix(y_test, test_predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Classification Report
class_report = classification_report(y_test, test_predictions)
print("\nClassification Report:")
print(class_report)

"""I assumed that as Predicted...

**True Positive:** 1337

**True Negative:** 68

**False Positive:** 76

**False Negative:** 19

**Train data**
"""

from sklearn.metrics import confusion_matrix, classification_report

conf_matrix = confusion_matrix(y_train, train_predictions)
print("Confusion Matrix:")
print(conf_matrix)

# Classification Report
class_report = classification_report(y_train, train_predictions)
print("\nClassification Report:")
print(class_report)\

"""In Prevoius year we got 9.6%(480) success ratio.

In this year I predicted that 79.44%(3972) success ratio.

**THE END**
"""